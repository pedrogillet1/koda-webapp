/**
 * Reindex Lone Mountain Ranch xlsx document in Pinecone
 */
import { config } from 'dotenv';
config();

import { Pinecone } from '@pinecone-database/pinecone';
import OpenAI from 'openai';
import prisma from './src/config/database';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const CHUNK_SIZE = 1500;
const CHUNK_OVERLAP = 200;

function chunkText(text: string, chunkSize: number = CHUNK_SIZE, overlap: number = CHUNK_OVERLAP): string[] {
  const chunks: string[] = [];
  let start = 0;

  while (start < text.length) {
    const end = Math.min(start + chunkSize, text.length);
    const chunk = text.slice(start, end);

    if (chunk.trim().length > 50) { // Skip very short chunks
      chunks.push(chunk.trim());
    }

    start += chunkSize - overlap;
  }

  return chunks;
}

async function reindexDocument() {
  console.log('üîÑ Reindexing Lone Mountain Ranch xlsx document\n');

  // Find the xlsx documents
  const docs = await prisma.document.findMany({
    where: {
      filename: { contains: 'Lone Mountain' },
      mimeType: 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
      status: 'completed'
    },
    include: {
      metadata: true
    }
  });

  console.log(`Found ${docs.length} xlsx documents`);

  if (docs.length === 0) {
    console.log('‚ùå No xlsx documents found');
    await prisma.$disconnect();
    return;
  }

  const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY! });
  const index = pinecone.index(process.env.PINECONE_INDEX || 'koda-openai');

  for (const doc of docs) {
    console.log(`\nüìÑ Processing: ${doc.filename} (ID: ${doc.id})`);

    const extractedText = doc.metadata?.extractedText;
    if (!extractedText) {
      console.log('  ‚ùå No extracted text found');
      continue;
    }

    console.log(`  üìù Extracted text length: ${extractedText.length} chars`);

    // Chunk the text
    const chunks = chunkText(extractedText);
    console.log(`  üì¶ Created ${chunks.length} chunks`);

    // First, delete any existing vectors for this document
    console.log(`  üóëÔ∏è Deleting existing vectors for document...`);
    try {
      // We can't delete by filter in Pinecone starter, so we'll just add new ones
      // The IDs will be unique so old orphaned ones won't cause issues
    } catch (e) {
      console.log(`  ‚ö†Ô∏è Could not delete existing vectors (this is OK)`);
    }

    // Generate embeddings and upsert to Pinecone
    console.log(`  üî¢ Generating embeddings...`);

    const batchSize = 100;
    let totalUpserted = 0;

    for (let i = 0; i < chunks.length; i += batchSize) {
      const batchChunks = chunks.slice(i, i + batchSize);

      // Generate embeddings for batch
      const embeddingResponse = await openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: batchChunks,
      });

      // Prepare vectors for upsert
      const vectors = embeddingResponse.data.map((emb, j) => {
        const chunkIndex = i + j;
        return {
          id: `${doc.id}-chunk-${chunkIndex}`,
          values: emb.embedding,
          metadata: {
            documentId: doc.id,
            userId: doc.userId,
            filename: doc.filename,
            content: batchChunks[j],
            text: batchChunks[j],
            chunkIndex: chunkIndex,
            sourceType: 'xlsx',
          }
        };
      });

      // Upsert to Pinecone
      await index.upsert(vectors);
      totalUpserted += vectors.length;
      console.log(`  ‚úÖ Upserted batch ${Math.floor(i / batchSize) + 1}: ${vectors.length} vectors (total: ${totalUpserted})`);
    }

    console.log(`  üéâ Successfully indexed ${totalUpserted} chunks for ${doc.filename}`);
  }

  // Verify the indexing
  console.log('\n=== Verifying Index ===');
  const testQuery = "Room Revenue January 2025";
  const embeddingResponse = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: testQuery,
  });

  const results = await index.query({
    vector: embeddingResponse.data[0].embedding,
    topK: 5,
    filter: { userId: docs[0].userId },
    includeMetadata: true,
  });

  console.log(`\nQuery: "${testQuery}"`);
  console.log(`Results: ${results.matches?.length || 0}`);
  results.matches?.slice(0, 3).forEach((match, i) => {
    const m = match.metadata as any;
    console.log(`\n[${i + 1}] Score: ${match.score?.toFixed(4)}`);
    console.log(`    Document: ${m?.filename}`);
    console.log(`    Content: ${(m?.content || m?.text || '').substring(0, 200)}...`);
  });

  await prisma.$disconnect();
  console.log('\n‚úÖ Reindexing complete');
}

reindexDocument().catch(console.error);
