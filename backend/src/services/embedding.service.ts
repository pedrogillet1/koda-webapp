/**
 * Embedding Service
 * Generate vector embeddings using OpenAI API
 * - Uses text-embedding-3-small model (1536 dimensions)
 * - Batch processing support
 * - Rate limiting and retry logic
 * - Caching for repeated texts
 *
 * ‚úÖ SWITCHED FROM GOOGLE GEMINI TO OPENAI
 * Reason: Google's embedding API has frequent 500 errors
 * OpenAI provides better reliability and performance
 */

import OpenAI from 'openai';
import { config } from '../config/env';
import cacheService from './cache.service';

interface EmbeddingResult {
  text: string;
  embedding: number[];
  dimensions: number;
  model: string;
}

interface BatchEmbeddingResult {
  embeddings: EmbeddingResult[];
  totalProcessed: number;
  failedCount: number;
  processingTime: number;
}

interface EmbeddingOptions {
  taskType?: 'RETRIEVAL_QUERY' | 'RETRIEVAL_DOCUMENT' | 'SEMANTIC_SIMILARITY' | 'CLASSIFICATION';
  title?: string; // Optional title for the text
}

export class EmbeddingService {
  private openai: OpenAI;
  private readonly EMBEDDING_MODEL = 'text-embedding-3-small';
  private readonly EMBEDDING_DIMENSIONS = 1536; // OpenAI text-embedding-3-small
  private readonly MAX_BATCH_SIZE = 2048; // OpenAI allows up to 2048 in a single request
  private readonly MAX_TEXT_LENGTH = 8191; // OpenAI token limit (approx 32k characters)

  constructor() {
    if (!config.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY is not configured');
    }
    this.openai = new OpenAI({
      apiKey: config.OPENAI_API_KEY,
    });
  }

  /**
   * Generate embedding for a single text
   * Uses embeddingCache for 150x faster repeated queries!
   */
  async generateEmbedding(
    text: string,
    options: EmbeddingOptions = {},
    retryCount: number = 0
  ): Promise<EmbeddingResult> {
    const maxRetries = 3;
    console.time('‚ö° Embedding Generation');

    try {
      // Preprocess text first
      const processedText = this.preprocessText(text);

      // Check cache first using the optimized embedding cache service
      const cachedEmbedding = await cacheService.getCachedEmbedding(processedText);
      if (cachedEmbedding) {
        console.timeEnd('‚ö° Embedding Generation');
        return {
          text: processedText,
          embedding: cachedEmbedding,
          dimensions: this.EMBEDDING_DIMENSIONS,
          model: this.EMBEDDING_MODEL,
        };
      }

      console.log(`üîÆ [Embedding Service] Generating OpenAI embedding for text (${processedText.length} chars)...`);

      // Generate embedding with OpenAI
      const response = await this.openai.embeddings.create({
        model: this.EMBEDDING_MODEL,
        input: processedText,
      });

      const embedding = response.data[0].embedding;

      if (!embedding || embedding.length === 0) {
        throw new Error('Empty embedding returned from API');
      }

      // Cache the result using the optimized cache service
      await cacheService.cacheEmbedding(processedText, embedding);

      console.log(`‚úÖ [Embedding Service] Generated ${embedding.length}-dimensional OpenAI embedding`);
      console.timeEnd('‚ö° Embedding Generation');

      return {
        text: processedText,
        embedding,
        dimensions: embedding.length,
        model: this.EMBEDDING_MODEL,
      };
    } catch (error: any) {
      console.timeEnd('‚ö° Embedding Generation');

      // Handle rate limiting with exponential backoff
      if (error.status === 429 || (error.message && error.message.includes('429'))) {
        if (retryCount < maxRetries) {
          const backoffDelay = Math.pow(2, retryCount) * 1000; // 1s, 2s, 4s
          console.warn(`‚è≥ [Embedding Service] Rate limit hit (429). Retrying in ${backoffDelay}ms (attempt ${retryCount + 1}/${maxRetries})...`);
          await this.sleep(backoffDelay);
          return this.generateEmbedding(text, options, retryCount + 1);
        } else {
          console.error('‚ùå [Embedding Service] Max retries reached for rate limiting');
        }
      }

      // Handle quota exceeded
      if (error.status === 429 || (error.message && (error.message.includes('quota') || error.message.includes('insufficient_quota')))) {
        console.error('üí∞ [Embedding Service] API quota exceeded. Please check your OpenAI API usage.');
        throw new Error('API quota exceeded. Please try again later or upgrade your API plan.');
      }

      console.error('‚ùå [Embedding Service] Error:', error);
      throw new Error(`Failed to generate embedding: ${error.message}`);
    }
  }

  /**
   * Generate embeddings for multiple texts in batch
   * ‚úÖ OPTIMIZED: Check cache BEFORE API call (10-20s ‚Üí 1-2s for cached texts)
   */
  async generateBatchEmbeddings(
    texts: string[],
    options: EmbeddingOptions = {}
  ): Promise<BatchEmbeddingResult> {
    const startTime = Date.now();
    const embeddings: EmbeddingResult[] = [];
    let failedCount = 0;
    let cacheHits = 0;
    let cacheMisses = 0;

    console.log(`‚ö° [Embedding Service] BATCH generating embeddings for ${texts.length} texts...`);

    // Preprocess all texts
    const processedTexts = texts.map(t => this.preprocessText(t));

    // ‚úÖ CRITICAL OPTIMIZATION: Check cache for ALL texts BEFORE making any API calls
    console.log(`   üîç Checking cache for ${processedTexts.length} texts...`);
    const cacheCheckStart = Date.now();

    const cacheResults = await Promise.all(
      processedTexts.map(async (text) => ({
        text,
        cached: await cacheService.getCachedEmbedding(text)
      }))
    );

    const cacheCheckTime = Date.now() - cacheCheckStart;
    console.log(`   ‚úÖ Cache check completed in ${cacheCheckTime}ms`);

    // Separate cached and uncached texts
    const cachedEmbeddings = new Map<string, number[]>();
    const uncachedTexts: string[] = [];

    for (const result of cacheResults) {
      if (result.cached) {
        cachedEmbeddings.set(result.text, result.cached);
        cacheHits++;
      } else {
        uncachedTexts.push(result.text);
        cacheMisses++;
      }
    }

    console.log(`   üìä Cache stats: ${cacheHits} hits, ${cacheMisses} misses (${((cacheHits / processedTexts.length) * 100).toFixed(1)}% hit rate)`);

    // Add cached embeddings to results (preserving original order)
    const textToIndexMap = new Map<string, number>();
    processedTexts.forEach((text, index) => textToIndexMap.set(text, index));
    const orderedEmbeddings: (EmbeddingResult | null)[] = new Array(processedTexts.length).fill(null);

    for (const [text, embedding] of cachedEmbeddings.entries()) {
      const index = textToIndexMap.get(text)!;
      orderedEmbeddings[index] = {
        text,
        embedding,
        dimensions: this.EMBEDDING_DIMENSIONS,
        model: this.EMBEDDING_MODEL,
      };
    }

    // Only call API for uncached texts
    if (uncachedTexts.length > 0) {
      console.log(`   üåê Calling OpenAI API for ${uncachedTexts.length} uncached texts...`);

      // Process uncached texts in batches of 2048 (OpenAI API limit)
      const batches = this.createBatches(uncachedTexts, this.MAX_BATCH_SIZE);

      for (let i = 0; i < batches.length; i++) {
        const batch = batches[i];
        console.log(`   üì¶ Processing batch ${i + 1}/${batches.length} (${batch.length} texts in ONE API call)...`);

        try {
          // ‚ö° Send ALL uncached texts in ONE API call
          const response = await this.openai.embeddings.create({
            model: this.EMBEDDING_MODEL,
            input: batch,
          });

          // Process results and cache them
          for (let j = 0; j < batch.length; j++) {
            const embeddingValues = response.data[j].embedding;
            const text = batch[j];
            const index = textToIndexMap.get(text)!;

            if (!embeddingValues || embeddingValues.length === 0) {
              console.error(`   ‚ùå Empty embedding for text ${j} in batch ${i + 1}`);
              failedCount++;
              orderedEmbeddings[index] = {
                text,
                embedding: new Array(this.EMBEDDING_DIMENSIONS).fill(0),
                dimensions: this.EMBEDDING_DIMENSIONS,
                model: this.EMBEDDING_MODEL,
              };
            } else {
              // Cache the result for future use
              await cacheService.cacheEmbedding(text, embeddingValues);

              orderedEmbeddings[index] = {
                text,
                embedding: embeddingValues,
                dimensions: embeddingValues.length,
                model: this.EMBEDDING_MODEL,
              };
            }
          }

          console.log(`   ‚úÖ Batch ${i + 1}/${batches.length} completed (${batch.length} embeddings)`);

        } catch (error: any) {
          console.error(`   ‚ùå Batch ${i + 1} failed: ${error.message}`);
          failedCount += batch.length;

          // Add placeholder embeddings for failed batch
          for (const text of batch) {
            const index = textToIndexMap.get(text)!;
            orderedEmbeddings[index] = {
              text,
              embedding: new Array(this.EMBEDDING_DIMENSIONS).fill(0),
              dimensions: this.EMBEDDING_DIMENSIONS,
              model: this.EMBEDDING_MODEL,
            };
          }
        }

        // Small delay between batches to avoid rate limiting
        if (i < batches.length - 1) {
          await this.sleep(100); // 100ms delay
        }
      }
    } else {
      console.log(`   ‚úÖ All ${processedTexts.length} embeddings found in cache - no API calls needed!`);
    }

    const processingTime = Date.now() - startTime;

    console.log(`‚úÖ [Embedding Service] Generated ${processedTexts.length} OpenAI embeddings in ${(processingTime / 1000).toFixed(2)}s`);
    console.log(`   üìä Performance: ${cacheHits} from cache, ${cacheMisses} from API`);
    if (failedCount > 0) {
      console.warn(`   ‚ö†Ô∏è ${failedCount} embeddings failed`);
    }

    return {
      embeddings: orderedEmbeddings.filter((e): e is EmbeddingResult => e !== null),
      totalProcessed: texts.length,
      failedCount,
      processingTime,
    };
  }

  /**
   * Generate embedding optimized for query (search)
   */
  async generateQueryEmbedding(query: string): Promise<EmbeddingResult> {
    return this.generateEmbedding(query, {
      taskType: 'RETRIEVAL_QUERY',
      title: 'User Query',
    });
  }

  /**
   * Generate embedding optimized for document storage
   */
  async generateDocumentEmbedding(text: string, title?: string): Promise<EmbeddingResult> {
    return this.generateEmbedding(text, {
      taskType: 'RETRIEVAL_DOCUMENT',
      title,
    });
  }

  /**
   * Calculate cosine similarity between two embeddings
   */
  calculateSimilarity(embedding1: number[], embedding2: number[]): number {
    if (embedding1.length !== embedding2.length) {
      throw new Error('Embeddings must have the same dimensions');
    }

    let dotProduct = 0;
    let norm1 = 0;
    let norm2 = 0;

    for (let i = 0; i < embedding1.length; i++) {
      dotProduct += embedding1[i] * embedding2[i];
      norm1 += embedding1[i] * embedding1[i];
      norm2 += embedding2[i] * embedding2[i];
    }

    return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
  }

  /**
   * Find most similar embeddings to a query embedding
   */
  findTopKSimilar(
    queryEmbedding: number[],
    candidateEmbeddings: Array<{ id: string; embedding: number[]; metadata?: any }>,
    k: number = 10
  ): Array<{ id: string; similarity: number; metadata?: any }> {
    const similarities = candidateEmbeddings.map(candidate => ({
      id: candidate.id,
      similarity: this.calculateSimilarity(queryEmbedding, candidate.embedding),
      metadata: candidate.metadata,
    }));

    // Sort by similarity (descending) and take top K
    return similarities
      .sort((a, b) => b.similarity - a.similarity)
      .slice(0, k);
  }

  /**
   * Preprocess text before embedding
   */
  private preprocessText(text: string): string {
    // Remove excessive whitespace
    let processed = text.replace(/\s+/g, ' ').trim();

    // Truncate if too long
    if (processed.length > this.MAX_TEXT_LENGTH) {
      console.warn(`   ‚ö†Ô∏è Text truncated from ${processed.length} to ${this.MAX_TEXT_LENGTH} chars`);
      processed = processed.slice(0, this.MAX_TEXT_LENGTH);
    }

    return processed;
  }


  /**
   * Create batches from array
   */
  private createBatches<T>(array: T[], batchSize: number): T[][] {
    const batches: T[][] = [];
    for (let i = 0; i < array.length; i += batchSize) {
      batches.push(array.slice(i, i + batchSize));
    }
    return batches;
  }

  /**
   * Sleep utility for rate limiting
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Clear embedding cache (delegates to cacheService)
   */
  async clearCache(): Promise<void> {
    await cacheService.clearAll();
  }

  /**
   * Get cache statistics (delegates to cacheService)
   */
  async getCacheStats() {
    return await cacheService.getCacheStats();
  }

  /**
   * Get embedding dimensions for this model
   */
  getEmbeddingDimensions(): number {
    return this.EMBEDDING_DIMENSIONS;
  }
}

// Infrastructure singleton - kept for backward compatibility
// Can also be accessed via container.getEmbedding()
export default new EmbeddingService();
