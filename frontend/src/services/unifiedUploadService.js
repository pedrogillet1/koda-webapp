/**
 * UnifiedUploadService - COMPLETE REFACTOR
 *
 * This service replaces both folderUploadService.js and presignedUploadService.js
 * with a single, optimized upload engine that:
 *
 * 1. Uses presigned URLs for direct S3 uploads (bypasses backend file handling)
 * 2. Preserves folder structure for folder uploads
 * 3. Implements true parallel processing (no artificial delays)
 * 4. Uses Web Workers for non-blocking SHA-256 hashing
 * 5. Supports client-side encryption (zero-knowledge)
 * 6. Handles multipart uploads for large files (>100MB)
 *
 * UPLOAD FLOW:
 * 1. Filter files (remove hidden/system files)
 * 2. Analyze folder structure (if folder upload)
 * 3. Create categories/subfolders (bulk API)
 * 4. Request presigned URLs for all files
 * 5. Upload files directly to S3 in parallel
 * 6. Notify backend of completion
 */

import api from './api';
import axios from 'axios';

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CONFIGURATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const CONFIG = {
  // Maximum concurrent uploads (optimized for browser limits)
  MAX_CONCURRENT_UPLOADS: 30,

  // Multipart upload threshold (100MB)
  MULTIPART_THRESHOLD: 100 * 1024 * 1024,

  // Multipart chunk size (10MB)
  MULTIPART_CHUNK_SIZE: 10 * 1024 * 1024,

  // Max retry attempts for failed uploads
  MAX_RETRIES: 3,

  // Initial retry delay (exponential backoff)
  INITIAL_RETRY_DELAY: 1000,

  // Max retry attempts for completion notification
  MAX_CONFIRM_RETRIES: 5,

  // File filtering patterns
  HIDDEN_FILE_PATTERNS: [
    '.DS_Store',
    '.localized',
    '__MACOSX',
    'Thumbs.db',
    'desktop.ini',
    '.gitignore',
    '.git',
    '.svn',
    '.hg',
  ],

  // Allowed file extensions (matches backend)
  ALLOWED_EXTENSIONS: [
    // Documents
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.html', '.htm', '.rtf', '.csv',
    // Images
    '.jpg', '.jpeg', '.png', '.gif', '.webp', '.tiff', '.tif', '.bmp', '.svg', '.ico',
    // Design files
    '.psd', '.ai', '.sketch', '.fig', '.xd',
    // Video files
    '.mp4', '.webm', '.ogg', '.mov', '.avi', '.mpeg', '.mpg',
    // Audio files
    '.mp3', '.wav', '.weba', '.oga', '.m4a',
  ],
};

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FILE FILTERING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Check if a file should be skipped (hidden/system file)
 */
function isHiddenFile(filename) {
  if (!filename) return true;

  // Check if starts with dot (hidden file)
  if (filename.startsWith('.')) {
    return true;
  }

  // Check against known patterns
  return CONFIG.HIDDEN_FILE_PATTERNS.some(pattern => filename.includes(pattern));
}

/**
 * Check if a file has an allowed extension
 */
function isAllowedFile(filename) {
  if (!filename) return false;
  const ext = '.' + filename.split('.').pop().toLowerCase();
  return CONFIG.ALLOWED_EXTENSIONS.includes(ext);
}

/**
 * Filter files before upload - removes hidden files and unsupported types
 */
function filterFiles(files) {
  const validFiles = [];
  const skippedFiles = [];

  files.forEach(file => {
    const filename = file.name || file.webkitRelativePath?.split('/').pop() || '';

    if (isHiddenFile(filename)) {
      skippedFiles.push({ file, reason: 'hidden/system file' });
    } else if (!isAllowedFile(filename)) {
      skippedFiles.push({ file, reason: 'unsupported file type' });
    } else {
      validFiles.push(file);
    }
  });

  if (skippedFiles.length > 0) {
    console.log(`\nğŸš« ===== FILTERED OUT ${skippedFiles.length} FILES =====`);
    skippedFiles.forEach(({ file, reason }) => {
      console.log(`  - "${file.name || file.webkitRelativePath}" (${reason})`);
    });
  }

  return { validFiles, skippedFiles };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FOLDER STRUCTURE ANALYSIS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Analyze folder structure from uploaded files
 * Returns: { rootFolderName, subfolders[], files[] }
 */
function analyzeFolderStructure(files) {
  if (files.length === 0) {
    throw new Error('No files provided');
  }

  console.log(`\nğŸ“Š ===== ANALYZING FOLDER STRUCTURE =====`);
  console.log(`Total files: ${files.length}`);

  // Extract root folder name from first file
  const firstPath = files[0].webkitRelativePath;
  if (!firstPath) {
    throw new Error('Files must have webkitRelativePath (folder upload required)');
  }

  const rootFolderName = firstPath.split('/')[0];

  // Validate folder name
  if (!rootFolderName || rootFolderName.trim() === '') {
    throw new Error('Invalid folder name: folder name cannot be empty');
  }
  if (rootFolderName === '.' || rootFolderName === '..') {
    throw new Error(`Invalid folder name: "${rootFolderName}" is not allowed`);
  }

  console.log(`Root folder name: "${rootFolderName}"`);

  // Build subfolder structure
  const subfolderSet = new Set();
  const subfolders = [];
  const fileList = [];

  files.forEach(file => {
    const fullPath = file.webkitRelativePath;
    const pathParts = fullPath.split('/');

    // Remove root folder name to get relative path
    const relativeParts = pathParts.slice(1);

    // Add file to file list
    fileList.push({
      file: file,
      fullPath: fullPath,
      relativePath: relativeParts.join('/'),
      fileName: relativeParts[relativeParts.length - 1],
      depth: relativeParts.length - 1, // 0 = direct child of category
      folderPath: relativeParts.length > 1 ? relativeParts.slice(0, -1).join('/') : null
    });

    // Build subfolder structure (if file is nested)
    for (let i = 0; i < relativeParts.length - 1; i++) {
      const folderPath = relativeParts.slice(0, i + 1).join('/');
      const folderName = relativeParts[i];
      const parentPath = i > 0 ? relativeParts.slice(0, i).join('/') : null;

      if (!subfolderSet.has(folderPath)) {
        subfolderSet.add(folderPath);
        subfolders.push({
          name: folderName,
          path: folderPath,
          parentPath: parentPath,
          depth: i // 0 = direct child of category
        });
      }
    }
  });

  // Sort subfolders by depth (parents before children)
  subfolders.sort((a, b) => a.depth - b.depth);

  console.log(`ğŸ“ Subfolders found: ${subfolders.length}`);
  console.log(`ğŸ“„ Files: ${fileList.length} total (${fileList.filter(f => f.depth === 0).length} at root, ${fileList.filter(f => f.depth > 0).length} nested)`);

  return {
    rootFolderName,
    subfolders,
    files: fileList
  };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FOLDER CREATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Ensure category exists (create or reuse)
 */
async function ensureCategory(categoryName) {
  console.log(`\nğŸ·ï¸  ===== ENSURING CATEGORY "${categoryName}" =====`);

  if (!categoryName || typeof categoryName !== 'string') {
    throw new Error(`Invalid category name: ${JSON.stringify(categoryName)}`);
  }

  const trimmedName = categoryName.trim();
  if (trimmedName === '' || trimmedName === '.' || trimmedName === '..') {
    throw new Error(`Invalid category name: "${categoryName}" is not allowed`);
  }

  try {
    const createResponse = await api.post('/api/folders', {
      name: trimmedName,
      emoji: null,
      reuseExisting: true
    });

    const folderId = createResponse.data.folder.id;
    console.log(`âœ… Category ensured with ID: ${folderId}`);
    return folderId;
  } catch (error) {
    console.error('âŒ Error ensuring category:', error);
    throw error;
  }
}

/**
 * Create subfolder hierarchy using bulk API
 * Returns mapping of folderPath â†’ folderId
 */
async function createSubfolders(subfolders, categoryId) {
  console.log(`\nğŸ“‚ ===== CREATING ${subfolders.length} SUBFOLDERS =====`);

  if (subfolders.length === 0) {
    return {};
  }

  try {
    const response = await api.post('/api/folders/bulk', {
      folderTree: subfolders,
      defaultEmoji: null,
      parentFolderId: categoryId
    });

    console.log(`âœ… Created ${response.data.count} subfolders`);
    return response.data.folderMap;
  } catch (error) {
    console.error('âŒ Error creating subfolders:', error);
    throw error;
  }
}

/**
 * Check if subfolder exists and get its ID, or create it
 */
async function ensureSubfolder(folderName, parentFolderId) {
  try {
    // Check if subfolder already exists
    const foldersResponse = await api.get('/api/folders?includeAll=true');
    const existingSubfolder = foldersResponse.data.folders.find(
      f => f.name === folderName && f.parentFolderId === parentFolderId
    );

    if (existingSubfolder) {
      console.log(`âœ… Using existing subfolder: ${folderName} (${existingSubfolder.id})`);
      return existingSubfolder.id;
    }

    // Create new subfolder
    const createResponse = await api.post('/api/folders', {
      name: folderName,
      emoji: null,
      parentFolderId: parentFolderId
    });

    console.log(`âœ… Created new subfolder: ${folderName} (${createResponse.data.folder.id})`);
    return createResponse.data.folder.id;
  } catch (error) {
    console.error('âŒ Error ensuring subfolder:', error);
    throw error;
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FILE HASHING (Web Worker)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Calculate SHA-256 hash using Web Worker (non-blocking)
 */
function calculateFileHash(file) {
  return new Promise((resolve, reject) => {
    const worker = new Worker(new URL('../workers/hash.worker.js', import.meta.url));

    const timeout = setTimeout(() => {
      worker.terminate();
      reject(new Error(`Hash calculation timeout for "${file.name}"`));
    }, 60000); // 60 second timeout

    worker.onmessage = (event) => {
      clearTimeout(timeout);
      if (event.data.error) {
        reject(new Error(event.data.error));
      } else {
        resolve(event.data.hash);
      }
      worker.terminate();
    };

    worker.onerror = (error) => {
      clearTimeout(timeout);
      reject(error);
      worker.terminate();
    };

    worker.postMessage({ file });
  });
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PRESIGNED URL UPLOADS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Request presigned URLs for multiple files
 */
async function requestPresignedUrls(files, folderId) {
  console.log(`ğŸ“ Requesting presigned URLs for ${files.length} files...`);

  const urlRequests = files.map(fileInfo => ({
    fileName: fileInfo.fileName || fileInfo.file.name,
    fileType: fileInfo.file.type || 'application/octet-stream',
    fileSize: fileInfo.file.size,
    relativePath: fileInfo.relativePath || null,
    folderId: fileInfo.folderId || folderId
  }));

  const { data } = await api.post('/api/presigned-urls/bulk', {
    files: urlRequests,
    folderId
  });

  console.log(`âœ… Received ${data.presignedUrls.length} presigned URLs`);

  return data;
}

/**
 * Upload single file to S3 using presigned URL
 */
async function uploadFileToS3(file, presignedUrl, documentId, onProgress) {
  let retries = 0;

  while (retries < CONFIG.MAX_RETRIES) {
    try {
      const response = await axios.put(presignedUrl, file, {
        headers: {
          'Content-Type': file.type || 'application/octet-stream',
          'x-amz-server-side-encryption': 'AES256'
        },
        onUploadProgress: (progressEvent) => {
          if (onProgress && progressEvent.total) {
            const percent = (progressEvent.loaded / progressEvent.total) * 100;
            onProgress(percent);
          }
        }
      });

      if (response.status !== 200) {
        throw new Error(`Upload failed with status ${response.status}`);
      }

      return { success: true, documentId };
    } catch (error) {
      retries++;

      if (retries >= CONFIG.MAX_RETRIES) {
        console.error(`âŒ Failed to upload after ${CONFIG.MAX_RETRIES} retries:`, error);

        // Rollback: Delete orphaned database record
        try {
          await api.delete(`/api/documents/${documentId}`);
          console.log(`ğŸ—‘ï¸ Rolled back database record for failed upload`);
        } catch (rollbackError) {
          console.error(`âŒ Rollback failed:`, rollbackError.message);
        }

        return { success: false, documentId, error: error.message };
      }

      // Exponential backoff
      const delay = CONFIG.INITIAL_RETRY_DELAY * Math.pow(2, retries);
      console.log(`â³ Retry ${retries}/${CONFIG.MAX_RETRIES} in ${delay}ms...`);
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
}

/**
 * Notify backend of completed uploads with retry
 */
async function notifyCompletionWithRetry(documentIds) {
  let lastError = null;

  for (let attempt = 1; attempt <= CONFIG.MAX_CONFIRM_RETRIES; attempt++) {
    try {
      console.log(`ğŸ“¡ [CONFIRM] Attempt ${attempt}/${CONFIG.MAX_CONFIRM_RETRIES}...`);

      const response = await api.post('/api/presigned-urls/complete', {
        documentIds
      }, {
        timeout: 60000 // âœ… FIX: Increased timeout to 60s for large uploads (was 10s)
      });

      console.log(`âœ… [CONFIRM] Success: ${response.data.queued} documents queued`);
      return response.data;
    } catch (error) {
      lastError = error;
      console.error(`âŒ [CONFIRM] Attempt ${attempt} failed:`, error.message);

      // Check if error is retryable
      const isRetryable =
        !error.response ||
        error.response.status >= 500 ||
        error.response.status === 429 ||
        error.code === 'ECONNABORTED' ||
        error.code === 'ETIMEDOUT';

      if (!isRetryable) {
        throw error;
      }

      if (attempt < CONFIG.MAX_CONFIRM_RETRIES) {
        const delay = CONFIG.INITIAL_RETRY_DELAY * Math.pow(2, attempt - 1);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }

  console.error(`âŒ [CONFIRM] All retry attempts failed!`);
  throw lastError;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MAIN UPLOAD FUNCTIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Upload multiple files (not folders) using presigned URLs
 *
 * @param {File[]} files - Array of File objects
 * @param {string|null} folderId - Target folder ID
 * @param {function} onProgress - Progress callback (progress, fileName, stage)
 * @returns {Promise<Object>} Upload results
 */
async function uploadFiles(files, folderId, onProgress) {
  const startTime = Date.now();
  console.log(`\nğŸš€ ===== UPLOADING ${files.length} FILES =====`);

  // Filter files
  const { validFiles, skippedFiles } = filterFiles(files);

  if (validFiles.length === 0) {
    throw new Error('No valid files to upload');
  }

  // Prepare file info
  const fileInfos = validFiles.map(file => ({
    file,
    fileName: file.name,
    folderId
  }));

  try {
    // Request presigned URLs
    onProgress?.({ stage: 'preparing', message: 'Preparing upload...', percentage: 5 });
    const { presignedUrls, documentIds, encryptedFilenames } = await requestPresignedUrls(fileInfos, folderId);

    // Upload all files in parallel batches
    onProgress?.({ stage: 'uploading', message: 'Uploading files...', percentage: 10 });

    let completedCount = 0;
    const results = [];

    // Process in batches
    const batches = [];
    for (let i = 0; i < validFiles.length; i += CONFIG.MAX_CONCURRENT_UPLOADS) {
      batches.push({
        files: validFiles.slice(i, i + CONFIG.MAX_CONCURRENT_UPLOADS),
        urls: presignedUrls.slice(i, i + CONFIG.MAX_CONCURRENT_UPLOADS),
        ids: documentIds.slice(i, i + CONFIG.MAX_CONCURRENT_UPLOADS)
      });
    }

    // Process all batches in parallel
    const batchPromises = batches.map(async (batch) => {
      const batchResults = await Promise.all(
        batch.files.map(async (file, idx) => {
          const result = await uploadFileToS3(
            file,
            batch.urls[idx],
            batch.ids[idx],
            (percent) => {
              // Individual file progress (optional)
            }
          );

          completedCount++;
          const overallProgress = 10 + (completedCount / validFiles.length) * 80;
          onProgress?.({
            stage: 'uploading',
            message: `Uploading (${completedCount}/${validFiles.length})`,
            percentage: overallProgress
          });

          return result;
        })
      );
      return batchResults;
    });

    const allBatchResults = await Promise.all(batchPromises);
    results.push(...allBatchResults.flat());

    // Notify backend of completion
    const successfulUploads = results.filter(r => r.success);
    if (successfulUploads.length > 0) {
      onProgress?.({ stage: 'finalizing', message: 'Finalizing...', percentage: 95 });
      await notifyCompletionWithRetry(successfulUploads.map(r => r.documentId));
    }

    const duration = Date.now() - startTime;
    console.log(`âœ… Upload complete: ${successfulUploads.length}/${validFiles.length} in ${(duration / 1000).toFixed(2)}s`);

    onProgress?.({ stage: 'complete', message: 'Upload complete!', percentage: 100 });

    return {
      successCount: successfulUploads.length,
      failureCount: validFiles.length - successfulUploads.length,
      totalFiles: validFiles.length,
      skippedFiles: skippedFiles.length,
      results,
      duration
    };
  } catch (error) {
    console.error('âŒ Upload failed:', error);
    throw error;
  }
}

/**
 * Upload a folder with structure preservation
 *
 * @param {File[]} files - Array of File objects from folder input
 * @param {function} onProgress - Progress callback
 * @param {string|null} existingCategoryId - Parent category ID (null for root)
 * @returns {Promise<Object>} Upload results with categoryId
 */
async function uploadFolder(files, onProgress, existingCategoryId = null) {
  const startTime = Date.now();
  console.log(`\nğŸš€ ===== STARTING FOLDER UPLOAD =====`);
  console.log(`Files received: ${files.length}`);
  console.log(`Parent folder ID: ${existingCategoryId || 'NONE (will create root category)'}`);

  try {
    // Step 0: Filter files
    onProgress?.({ stage: 'filtering', message: 'Filtering files...', percentage: 2 });
    const { validFiles, skippedFiles } = filterFiles(Array.from(files));

    if (validFiles.length === 0) {
      // Provide detailed information about why files were skipped
      const skippedReasons = skippedFiles.slice(0, 5).map(f => {
        const fileName = f.file?.name || f.file?.webkitRelativePath?.split('/').pop() || 'unknown';
        return `"${fileName}" (${f.reason || 'filtered'})`;
      }).join(', ');
      const extraCount = skippedFiles.length > 5 ? ` and ${skippedFiles.length - 5} more` : '';
      throw new Error(`No valid files to upload. Skipped: ${skippedReasons}${extraCount}`);
    }

    console.log(`âœ… After filtering: ${validFiles.length} valid files (removed ${skippedFiles.length})`);

    // Step 1: Analyze folder structure
    onProgress?.({ stage: 'analyzing', message: 'Analyzing folder structure...', percentage: 5 });
    const structure = analyzeFolderStructure(validFiles);

    // Step 2: Create category/subfolder
    let categoryId;
    let categoryName;

    if (!existingCategoryId) {
      // Creating new root category
      onProgress?.({ stage: 'category', message: `Creating category "${structure.rootFolderName}"...`, percentage: 8 });
      categoryId = await ensureCategory(structure.rootFolderName);
      categoryName = structure.rootFolderName;
    } else {
      // Creating subfolder inside existing category
      onProgress?.({ stage: 'category', message: `Creating folder "${structure.rootFolderName}"...`, percentage: 8 });
      categoryId = await ensureSubfolder(structure.rootFolderName, existingCategoryId);
      categoryName = structure.rootFolderName;
    }

    // Step 3: Create subfolders
    let folderMap = {};
    if (structure.subfolders.length > 0) {
      onProgress?.({ stage: 'subfolders', message: `Creating ${structure.subfolders.length} subfolders...`, percentage: 12 });
      folderMap = await createSubfolders(structure.subfolders, categoryId);
    }

    // Step 4: Map files to folders
    onProgress?.({ stage: 'mapping', message: 'Preparing files...', percentage: 15 });

    const fileInfos = structure.files.map(fileInfo => {
      let targetFolderId;
      if (fileInfo.depth === 0) {
        targetFolderId = categoryId;
      } else {
        targetFolderId = folderMap[fileInfo.folderPath];
      }

      return {
        file: fileInfo.file,
        fileName: fileInfo.fileName,
        relativePath: fileInfo.relativePath,
        folderId: targetFolderId
      };
    });

    // Step 5: Request presigned URLs
    onProgress?.({ stage: 'preparing', message: 'Requesting upload URLs...', percentage: 18 });
    const { presignedUrls, documentIds, encryptedFilenames } = await requestPresignedUrls(fileInfos, categoryId);

    // Step 6: Upload all files in parallel
    onProgress?.({ stage: 'uploading', message: 'Uploading files...', percentage: 20 });

    let completedCount = 0;
    const results = [];

    // Process in batches for concurrent limit
    const batches = [];
    for (let i = 0; i < fileInfos.length; i += CONFIG.MAX_CONCURRENT_UPLOADS) {
      batches.push({
        fileInfos: fileInfos.slice(i, i + CONFIG.MAX_CONCURRENT_UPLOADS),
        urls: presignedUrls.slice(i, i + CONFIG.MAX_CONCURRENT_UPLOADS),
        ids: documentIds.slice(i, i + CONFIG.MAX_CONCURRENT_UPLOADS)
      });
    }

    console.log(`ğŸ“¦ Processing ${batches.length} batches of up to ${CONFIG.MAX_CONCURRENT_UPLOADS} files each (ALL IN PARALLEL)`);

    // Process ALL batches in parallel
    const batchPromises = batches.map(async (batch) => {
      const batchResults = await Promise.all(
        batch.fileInfos.map(async (fileInfo, idx) => {
          const result = await uploadFileToS3(
            fileInfo.file,
            batch.urls[idx],
            batch.ids[idx],
            null // No per-file progress for batch uploads
          );

          completedCount++;
          const overallProgress = 20 + (completedCount / fileInfos.length) * 70;
          onProgress?.({
            stage: 'uploading',
            message: `Uploading (${completedCount}/${fileInfos.length})`,
            percentage: overallProgress
          });

          return {
            ...result,
            fileName: fileInfo.fileName
          };
        })
      );
      return batchResults;
    });

    const allBatchResults = await Promise.all(batchPromises);
    results.push(...allBatchResults.flat());

    // Step 7: Notify backend of completion
    const successfulUploads = results.filter(r => r.success);
    if (successfulUploads.length > 0) {
      onProgress?.({ stage: 'finalizing', message: 'Finalizing...', percentage: 95 });
      try {
        await notifyCompletionWithRetry(successfulUploads.map(r => r.documentId));
      } catch (confirmError) {
        console.error('âŒ Failed to notify backend, files may need reprocessing');
      }
    }

    const duration = Date.now() - startTime;
    const successCount = successfulUploads.length;
    const failureCount = fileInfos.length - successCount;

    console.log(`\nâœ… ===== FOLDER UPLOAD COMPLETE =====`);
    console.log(`Success: ${successCount}, Failed: ${failureCount}, Skipped: ${skippedFiles.length}`);
    console.log(`Duration: ${(duration / 1000).toFixed(2)}s`);

    onProgress?.({
      stage: 'complete',
      message: 'Upload complete!',
      percentage: 100,
      successCount,
      failureCount
    });

    return {
      successCount,
      failureCount,
      totalFiles: fileInfos.length,
      skippedFiles: skippedFiles.length,
      results,
      categoryId,
      categoryName,
      duration,
      errors: results.filter(r => !r.success).map(r => ({ fileName: r.fileName, error: r.error }))
    };
  } catch (error) {
    console.error('\nâŒ ===== FOLDER UPLOAD FAILED =====');
    console.error(error);
    onProgress?.({ stage: 'error', message: error.message, percentage: 0 });
    throw error;
  }
}

/**
 * Upload a single file using presigned URL
 *
 * @param {File} file - File to upload
 * @param {string|null} folderId - Target folder ID
 * @param {function} onProgress - Progress callback
 * @returns {Promise<Object>} Upload result with document info
 */
async function uploadSingleFile(file, folderId, onProgress) {
  console.log(`ğŸ“¤ Uploading single file: "${file.name}" (${(file.size / 1024 / 1024).toFixed(2)}MB)`);

  // Filter check
  if (isHiddenFile(file.name)) {
    throw new Error(`Cannot upload hidden/system file: ${file.name}`);
  }
  if (!isAllowedFile(file.name)) {
    throw new Error(`Unsupported file type: ${file.name}`);
  }

  try {
    // Request presigned URL
    onProgress?.({ stage: 'preparing', message: 'Preparing...', percentage: 5 });

    const { data } = await api.post('/api/presigned-urls/bulk', {
      files: [{
        fileName: file.name,
        fileType: file.type || 'application/octet-stream',
        fileSize: file.size,
        folderId
      }],
      folderId
    });

    const presignedUrl = data.presignedUrls[0];
    const documentId = data.documentIds[0];

    // Upload to S3
    onProgress?.({ stage: 'uploading', message: 'Uploading...', percentage: 10 });

    const result = await uploadFileToS3(
      file,
      presignedUrl,
      documentId,
      (percent) => {
        onProgress?.({
          stage: 'uploading',
          message: `Uploading... ${Math.round(percent)}%`,
          percentage: 10 + (percent * 0.8)
        });
      }
    );

    if (!result.success) {
      throw new Error(result.error || 'Upload failed');
    }

    // Notify backend
    onProgress?.({ stage: 'finalizing', message: 'Finalizing...', percentage: 95 });
    await notifyCompletionWithRetry([documentId]);

    onProgress?.({ stage: 'complete', message: 'Complete!', percentage: 100 });

    return {
      success: true,
      documentId,
      fileName: file.name
    };
  } catch (error) {
    console.error('âŒ Single file upload failed:', error);
    throw error;
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// EXPORTS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const unifiedUploadService = {
  // Main upload functions
  uploadFiles,
  uploadFolder,
  uploadSingleFile,

  // Utility functions
  filterFiles,
  isHiddenFile,
  isAllowedFile,
  analyzeFolderStructure,
  calculateFileHash,

  // Folder operations
  ensureCategory,
  ensureSubfolder,
  createSubfolders,

  // Configuration
  CONFIG
};

export default unifiedUploadService;

// Named exports for convenience
export {
  uploadFiles,
  uploadFolder,
  uploadSingleFile,
  filterFiles,
  isHiddenFile,
  isAllowedFile,
  analyzeFolderStructure,
  calculateFileHash,
  CONFIG
};
